{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRISM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRISM:\n",
    "    def __init__(self):\n",
    "        self.X_train = []\n",
    "        self.X_test = []\n",
    "        self.y_train = []\n",
    "        self.y_test = []\n",
    "        self.n_samples = 0\n",
    "        self.n_features = 0\n",
    "        self.csv_datasets = [\n",
    "            \"../../datasets/contact_lenses.csv\",\n",
    "            \"../../datasets/restaurant_customer_rating_feats.csv\",\n",
    "            \"../../datasets/mushrooms.csv\",\n",
    "            \"../../datasets/2015_happiness_report_mod.csv\",\n",
    "            \"../../datasets/biomechanical_orthopedic_feats.csv\"\n",
    "        ]\n",
    "        self.csv_datasets_col_names = [\n",
    "            ['user_id', 'place_id', 'rating', 'food_rating', 'service_rating'],\n",
    "            ['class', 'cap_shape', 'cap_surface', 'cap_color', 'bruises', 'odor',\n",
    "             'gill_attachment', 'gill_spacing', 'gill_size', 'gill_color', 'stalk_shape',\n",
    "             'stalk_root', 'stalk_surface_above_ring', 'stalk_surface_below_ring',\n",
    "             'stalk_color_above_ring', 'stalk_color_below_ring', 'veil_type', 'veil_color',\n",
    "             'ring_number', 'ring_type', 'spore_print_color', 'population', 'habitat'],\n",
    "            ['country', 'region', 'happiness_rank', 'happiness_score', 'standard_error',\n",
    "             'economy_gdp', 'family', 'health_life_expectancy', 'freedom', 'trust_gov_corruption',\n",
    "             'generosity', 'dystopia_residual'],\n",
    "            ['pelvic_incidence', 'pelvic_tilt', 'lumbar_lordosis_angle',\n",
    "             'sacral_slope', 'pelvic_radius', 'spondylolisthesis_degree', 'diagnostic']\n",
    "        ]\n",
    "\n",
    "    def repair_continuous_attributes(self, dataset, features):\n",
    "        # Converts continuous attributes from float to int\n",
    "        self.n_samples = dataset.shape[0]\n",
    "        self.n_features = dataset.shape[1] - 1\n",
    "\n",
    "        for feature in features:\n",
    "            if dataset[feature].dtype == np.float64:\n",
    "                dataset[feature] = dataset[feature].astype(int)\n",
    "\n",
    "    def load_csv_dataset(self, csv_path, feature_names):\n",
    "        # Loads and returns a CSV dataset with the provided column names\n",
    "        dataset = pd.read_csv(csv_path)\n",
    "        dataset.columns = feature_names\n",
    "        return dataset\n",
    "\n",
    "    def fix_dataset_missing_values(self, dataset):\n",
    "        # Replaces missing values with the most frequent value in each column\n",
    "        for column in dataset.columns:\n",
    "            dataset[column] = dataset[column].replace('?', np.NaN)\n",
    "            dataset[column] = dataset[column].fillna(dataset[column].value_counts().index[0])\n",
    "\n",
    "    def build_learning_sets(self, dataset, class_attr, train_size):\n",
    "        # Builds the train/test sets based on the specified train size\n",
    "        dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "        n_train = int(self.n_samples * train_size)\n",
    "        n_test = self.n_samples - n_train\n",
    "\n",
    "        dataset_ = dataset.copy(deep=True)\n",
    "        self.fix_dataset_missing_values(dataset_)\n",
    "\n",
    "        print(dataset_)\n",
    "\n",
    "        input(\"Continue\")\n",
    "\n",
    "        self.y_train = dataset_.loc[0:n_train, class_attr].copy(deep=True)\n",
    "        self.y_test = dataset_.loc[n_train + 1:self.n_samples, class_attr].copy(deep=True)\n",
    "\n",
    "        dataset_ = dataset_.drop(class_attr, axis=1)\n",
    "\n",
    "        self.X_train = dataset_.loc[0:n_train].copy(deep=True)\n",
    "        self.X_test = dataset_.loc[n_train + 1:self.n_samples].copy(deep=True)\n",
    "\n",
    "    def data_preprocessing(self):\n",
    "        # Performs data preprocessing steps\n",
    "        print('A) ::Processing CSV files::')\n",
    "        dataset = self.load_csv_dataset(self.csv_datasets[0], self.csv_datasets_col_names[0])\n",
    "\n",
    "        print('B) ::Repairing continuous attributes in Dataset::')\n",
    "        self.repair_continuous_attributes(dataset, dataset.columns)\n",
    "\n",
    "        print('C) ::Building train/test sets::')\n",
    "        self.build_learning_sets(dataset, dataset.columns[-1], 1.0)\n",
    "\n",
    "    def PRISM(self):\n",
    "        # Generates the PRISM rule set\n",
    "        prism_rule_set = []\n",
    "        for label in set(self.y_train):\n",
    "            instances = [i for i, val in enumerate(self.y_train) if val == label]\n",
    "\n",
    "            while instances:\n",
    "                rule = []\n",
    "                X_train_ = self.X_train.copy(deep=True)\n",
    "                instances_covered = []\n",
    "                perfect_rule = False\n",
    "                rule_precision = 0.0\n",
    "                rule_coverage = 0.0\n",
    "\n",
    "                while not perfect_rule and len(rule) < self.n_features + 1:\n",
    "                    optimal_selector = [(\"\",\"\")]\n",
    "                    optimal_selector_prec = [0.0, 0.0, 0.0]\n",
    "                    instances_covered = []\n",
    "\n",
    "                    for attribute in X_train_.columns:\n",
    "                        attr_column = X_train_.loc[:, attribute]\n",
    "\n",
    "                        for attr_value in set(attr_column):\n",
    "                            total_attr_values_instances = attr_column[(attr_column == attr_value)].index._values\n",
    "                            total_matches = len(total_attr_values_instances)\n",
    "                            positive_attr_values_instances = list(set(total_attr_values_instances) & set(instances))\n",
    "                            positive_matches = len(positive_attr_values_instances)\n",
    "\n",
    "                            precision = (1.0 * positive_matches) / total_matches\n",
    "                            coverage = (1.0 * positive_matches) / self.n_samples\n",
    "\n",
    "                            if precision > optimal_selector_prec[2]:\n",
    "                                optimal_selector = (attribute, attr_value)\n",
    "                                optimal_selector_prec[0] = positive_matches\n",
    "                                optimal_selector_prec[1] = total_matches\n",
    "                                optimal_selector_prec[2] = precision\n",
    "                                rule_precision = precision\n",
    "                                rule_coverage = coverage\n",
    "                                instances_covered = positive_attr_values_instances\n",
    "                            elif precision == optimal_selector_prec[2] and positive_matches > optimal_selector_prec[0]:\n",
    "                                optimal_selector = (attribute, attr_value)\n",
    "                                optimal_selector_prec[0] = positive_matches\n",
    "                                optimal_selector_prec[1] = total_matches\n",
    "                                optimal_selector_prec[2] = precision\n",
    "                                instances_covered = positive_attr_values_instances\n",
    "                                rule_precision = precision\n",
    "                                rule_coverage = coverage\n",
    "\n",
    "                    if 0.0 < optimal_selector_prec[2] < 1.0:\n",
    "                        rule.append(optimal_selector)\n",
    "                        selector = rule[-1]\n",
    "                        filtered_rows = X_train_[(X_train_[selector[0]] != selector[1])].index._values\n",
    "                        X_train_ = X_train_.drop(filtered_rows).copy(deep=True)\n",
    "                        X_train_ = X_train_.drop(selector[0], axis=1)\n",
    "\n",
    "                        if len(X_train_.columns) == 0:\n",
    "                            perfect_rule = True\n",
    "                            continue\n",
    "                    elif optimal_selector_prec[2] == 1.0:\n",
    "                        rule.append(optimal_selector)\n",
    "                        perfect_rule = True\n",
    "                        continue\n",
    "                    elif optimal_selector_prec[2] == 0.0:\n",
    "                        input(\"Unexpected case\")\n",
    "\n",
    "                instances = list(set(instances) - set(instances_covered))\n",
    "                rule.append(label)\n",
    "                rule.append([rule_precision, rule_coverage])\n",
    "\n",
    "                print(\"RULE FOUND\")\n",
    "                metrics = rule[-1]\n",
    "                print(\"Rule:\")\n",
    "                print(rule)\n",
    "                print(\"Rule-Precision: \" + str(metrics[0]))\n",
    "                print(\"Rule-Coverage: \" + str(metrics[1]))\n",
    "                print(\"\\n\")\n",
    "\n",
    "                prism_rule_set.append(rule)\n",
    "\n",
    "        return prism_rule_set\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A) ::Processing CSV files::\n",
      "B) ::Repairing continuous attributes in Dataset::\n",
      "C) ::Building train/test sets::\n",
      "         user_id      place_id rating food_rating service_rating\n",
      "0      prebiopic         miope     no     reduida            cap\n",
      "1           jove  hipermetrope     no      normal          toves\n",
      "2      prebiopic  hipermetrope     si      normal            cap\n",
      "3      prebiopic         miope     si      normal          dures\n",
      "4      prebiopic         miope     si     reduida            cap\n",
      "5   preprebiopic         miope     si     reduida            cap\n",
      "6      prebiopic         miope     no      normal            cap\n",
      "7           jove         miope     si     reduida            cap\n",
      "8      prebiopic  hipermetrope     si     reduida            cap\n",
      "9      prebiopic  hipermetrope     no     reduida            cap\n",
      "10          jove         miope     no      normal          toves\n",
      "11          jove         miope     no     reduida            cap\n",
      "12  preprebiopic         miope     no      normal          toves\n",
      "13          jove  hipermetrope     si      normal          dures\n",
      "14  preprebiopic  hipermetrope     no      normal          toves\n",
      "15     prebiopic  hipermetrope     no      normal          toves\n",
      "16  preprebiopic         miope     si      normal          dures\n",
      "17          jove         miope     si      normal          dures\n",
      "18          jove  hipermetrope     no     reduida            cap\n",
      "19          jove  hipermetrope     si     reduida            cap\n",
      "20  preprebiopic  hipermetrope     no     reduida            cap\n",
      "21  preprebiopic         miope     no     reduida            cap\n",
      "22  preprebiopic  hipermetrope     si     reduida            cap\n",
      "23  preprebiopic  hipermetrope     si      normal            cap\n",
      "RULE FOUND\n",
      "Rule:\n",
      "[('rating', 'si'), ('food_rating', 'normal'), ('place_id', 'miope'), 'dures', [1.0, 0.125]]\n",
      "Rule-Precision: 1.0\n",
      "Rule-Coverage: 0.125\n",
      "\n",
      "\n",
      "RULE FOUND\n",
      "Rule:\n",
      "[('user_id', 'jove'), ('place_id', 'hipermetrope'), ('rating', 'si'), ('food_rating', 'normal'), 'dures', [1.0, 0.041666666666666664]]\n",
      "Rule-Precision: 1.0\n",
      "Rule-Coverage: 0.041666666666666664\n",
      "\n",
      "\n",
      "RULE FOUND\n",
      "Rule:\n",
      "[('food_rating', 'reduida'), 'cap', [1.0, 0.5]]\n",
      "Rule-Precision: 1.0\n",
      "Rule-Coverage: 0.5\n",
      "\n",
      "\n",
      "RULE FOUND\n",
      "Rule:\n",
      "[('food_rating', 'normal'), ('user_id', 'prebiopic'), ('place_id', 'hipermetrope'), ('rating', 'si'), 'cap', [1.0, 0.041666666666666664]]\n",
      "Rule-Precision: 1.0\n",
      "Rule-Coverage: 0.041666666666666664\n",
      "\n",
      "\n",
      "RULE FOUND\n",
      "Rule:\n",
      "[('food_rating', 'normal'), ('user_id', 'preprebiopic'), ('place_id', 'hipermetrope'), ('rating', 'si'), 'cap', [1.0, 0.041666666666666664]]\n",
      "Rule-Precision: 1.0\n",
      "Rule-Coverage: 0.041666666666666664\n",
      "\n",
      "\n",
      "RULE FOUND\n",
      "Rule:\n",
      "[('user_id', 'prebiopic'), ('place_id', 'miope'), ('rating', 'no'), ('food_rating', 'normal'), 'cap', [1.0, 0.041666666666666664]]\n",
      "Rule-Precision: 1.0\n",
      "Rule-Coverage: 0.041666666666666664\n",
      "\n",
      "\n",
      "RULE FOUND\n",
      "Rule:\n",
      "[('rating', 'no'), ('food_rating', 'normal'), ('place_id', 'hipermetrope'), 'toves', [1.0, 0.125]]\n",
      "Rule-Precision: 1.0\n",
      "Rule-Coverage: 0.125\n",
      "\n",
      "\n",
      "RULE FOUND\n",
      "Rule:\n",
      "[('place_id', 'miope'), ('rating', 'no'), ('food_rating', 'normal'), ('user_id', 'preprebiopic'), 'toves', [1.0, 0.041666666666666664]]\n",
      "Rule-Precision: 1.0\n",
      "Rule-Coverage: 0.041666666666666664\n",
      "\n",
      "\n",
      "RULE FOUND\n",
      "Rule:\n",
      "[('user_id', 'jove'), ('place_id', 'miope'), ('rating', 'no'), ('food_rating', 'normal'), 'toves', [1.0, 0.041666666666666664]]\n",
      "Rule-Precision: 1.0\n",
      "Rule-Coverage: 0.041666666666666664\n",
      "\n",
      "\n",
      "%%%%%%%%%%%%%%%%% FINAL PRISM RULE SET %%%%%%%%%%%%%%%%%\n",
      "\n",
      "\n",
      "[('rating', 'si'), ('food_rating', 'normal'), ('place_id', 'miope'), 'dures', [1.0, 0.125]]\n",
      "[('user_id', 'jove'), ('place_id', 'hipermetrope'), ('rating', 'si'), ('food_rating', 'normal'), 'dures', [1.0, 0.041666666666666664]]\n",
      "[('food_rating', 'reduida'), 'cap', [1.0, 0.5]]\n",
      "[('food_rating', 'normal'), ('user_id', 'prebiopic'), ('place_id', 'hipermetrope'), ('rating', 'si'), 'cap', [1.0, 0.041666666666666664]]\n",
      "[('food_rating', 'normal'), ('user_id', 'preprebiopic'), ('place_id', 'hipermetrope'), ('rating', 'si'), 'cap', [1.0, 0.041666666666666664]]\n",
      "[('user_id', 'prebiopic'), ('place_id', 'miope'), ('rating', 'no'), ('food_rating', 'normal'), 'cap', [1.0, 0.041666666666666664]]\n",
      "[('rating', 'no'), ('food_rating', 'normal'), ('place_id', 'hipermetrope'), 'toves', [1.0, 0.125]]\n",
      "[('place_id', 'miope'), ('rating', 'no'), ('food_rating', 'normal'), ('user_id', 'preprebiopic'), 'toves', [1.0, 0.041666666666666664]]\n",
      "[('user_id', 'jove'), ('place_id', 'miope'), ('rating', 'no'), ('food_rating', 'normal'), 'toves', [1.0, 0.041666666666666664]]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    rbc = PRISM()\n",
    "    rbc.data_preprocessing()\n",
    "    rule_set = rbc.PRISM()\n",
    "\n",
    "    print(\"%%%%%%%%%%%%%%%%% FINAL PRISM RULE SET %%%%%%%%%%%%%%%%%\")\n",
    "    print(\"\\n\")\n",
    "    for prism_rule in rule_set:\n",
    "        print(prism_rule)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
