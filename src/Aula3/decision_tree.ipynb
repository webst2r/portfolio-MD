{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index # índice do atributo que será testado\n",
    "        self.threshold = threshold # valor de limite para o teste do atributo\n",
    "        self.left = left # sub-árvore à esquerda\n",
    "        self.right = right # sub-árvore à direita\n",
    "        self.value = value # valor da classe, caso seja um nó folha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, criterion='gini', prune=None, size=None, independence=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.criterion = criterion\n",
    "        self.prune = prune\n",
    "        self.size = size   # pre-prunning\n",
    "        self.independence = independence   # pre-prunning\n",
    "        self.tree = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes = len(set(y))\n",
    "        self.n_features = X.shape[1]\n",
    "        self.tree = self._grow_tree(X, y, depth=0)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "    \n",
    "    \n",
    "    def _predict(self, inputs):\n",
    "        node = self.tree\n",
    "        while node.value is None:\n",
    "            if inputs[node.feature_index] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.value\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Retorna a precisão do modelo no conjunto de dados fornecido.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = np.sum(y_pred == y) / len(y)\n",
    "        return accuracy\n",
    "\n",
    "    \n",
    "    def _get_leaf_nodes(self, node):\n",
    "        \"\"\"\n",
    "        Retorna todos os nós folha da subárvore enraizada em node.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            return []\n",
    "        if node.value is not None:\n",
    "            return [node]\n",
    "        leaf_nodes_left = self._get_leaf_nodes(node.left)\n",
    "        leaf_nodes_right = self._get_leaf_nodes(node.right)\n",
    "        return leaf_nodes_left + leaf_nodes_right\n",
    "    \n",
    "    def _get_nodes_list(self):\n",
    "        \"\"\"\n",
    "        Retorna uma lista com todos os nós da árvore.\n",
    "        \"\"\"\n",
    "        nodes_list = []\n",
    "        queue = collections.deque()\n",
    "        queue.append(self.tree)\n",
    "        while len(queue) > 0:\n",
    "            node = queue.popleft()\n",
    "            if node is not None:\n",
    "                nodes_list.append(node)\n",
    "                queue.append(node.left)\n",
    "                queue.append(node.right)\n",
    "        return nodes_list\n",
    "\n",
    "\n",
    "    def reduced_error_pruning(self, X_val, y_val):\n",
    "        if self.tree is None:\n",
    "            raise ValueError(\"You need to fit the model before pruning.\")\n",
    "        nodes = self._get_nodes_list()\n",
    "        best_accuracy = self.score(X_val, y_val)\n",
    "        for node in reversed(nodes):\n",
    "            node_left = node.left\n",
    "            node_right = node.right\n",
    "            node.left = None\n",
    "            node.right = None\n",
    "            accuracy = self.score(X_val, y_val)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                continue\n",
    "            node.left = node_left\n",
    "            node.right = node_right\n",
    "        return self\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Pre-prunning: max-depth, independence, size\n",
    "    Post-prunning: \n",
    "    Tanto o Reduced Error Pruning (REP) quanto o Pessimistic Error Pruning (PEP) são métodos post-prunning, o que significa que são aplicados após treinar a árvore de decisão.\n",
    "        1. Reduced Error Pruning (REP): criar uma lista de todos os nós da árvore e, em seguida, percorrer a lista de trás para frente. Em cada iteração, removeremos o nó da lista e avaliaremos a precisão da árvore no conjunto de validação. Se a precisão for melhor, o nó é removido da árvore.\n",
    "           Caso contrário, o nó é adicionado de volta à lista e a iteração continua para o próximo nó na lista.\n",
    "        \n",
    "        2. Pessimistic Error Pruning (PEP): adicionar uma nova variável de classe pep_limit que define a margem de erro máxima permitida.\n",
    "           Em seguida, vamos adicionar uma condição no método _grow_tree para calcular a margem de erro do nó interno antes de criar seus filhos.\n",
    "           Se a margem de erro do nó interno for maior do que o limite pep_limit, o nó será transformado em um nó folha com a classe mais frequente.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth=0, parent=None):\n",
    "\n",
    "        if depth == self.max_depth:   # adiciona a condição de profundidade máxima\n",
    "            return Node(value=self._most_common_class(y))\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if n_samples < self.min_samples_split or n_samples < self.min_samples_leaf:   # adiciona a condição de tamanho mínimo do nó\n",
    "            return Node(value=self._most_common_class(y))\n",
    "\n",
    "\n",
    "        # PRE-PRUNNING\n",
    "        if self.independence is not None and parent is not None:  # adiciona a condição de independência\n",
    "            if self._check_independence(X, parent):\n",
    "                return Node(value=self._most_common_class(y))\n",
    "                \n",
    "        if self.size is not None and depth > 0:  # adiciona a condição de tamanho máximo da árvore\n",
    "            if self._get_tree_size(X, y) >= self.size:\n",
    "                return Node(value=self._most_common_class(y))\n",
    "\n",
    "        best_feature_index, best_threshold = self._choose_best_feature(X, y)\n",
    "\n",
    "        left_idx = X[:, best_feature_index] <= best_threshold\n",
    "        X_left, y_left = X[left_idx], y[left_idx]\n",
    "        X_right, y_right = X[~left_idx], y[~left_idx]\n",
    "\n",
    "        left = self._grow_tree(X_left, y_left, depth=depth+1, parent=X)   # passa o nó pai como argumento\n",
    "        right = self._grow_tree(X_right, y_right, depth=depth+1, parent=X)   # passa o nó pai como argumento\n",
    "\n",
    "        return Node(feature_index=best_feature_index, threshold=best_threshold, left=left, right=right)\n",
    "    \n",
    "\n",
    "   \n",
    "    \"\"\"\n",
    "    Para implementar o pre-prunning de tamanho (size), precisamos verificar o tamanho da árvore antes de fazer uma divisão.\n",
    "    Se o tamanho atual da árvore for maior ou igual ao tamanho máximo definido pelo usuário, retornamos um nó folha com a classe mais frequente.\n",
    "    \"\"\"\n",
    "    def _get_tree_size(self, node):\n",
    "        if node is None:\n",
    "            return 0\n",
    "        return 1 + self._get_tree_size(node.left) + self._get_tree_size(node.right)\n",
    "    \n",
    "    def _gini_impurity(self, left_probabilities, right_probabilities, n_left, n_right):\n",
    "        # compute gini impurity\n",
    "        left_gini = 1 - sum(np.square(left_probabilities))\n",
    "        right_gini = 1 - sum(np.square(right_probabilities))\n",
    "        weighted_gini = (n_left * left_gini + n_right * right_gini) / (n_left + n_right)\n",
    "        return weighted_gini\n",
    "    \n",
    "\n",
    "    def _choose_best_feature(self, X, y):\n",
    "        if self.criterion == 'entropy':\n",
    "            best_feature_index, best_threshold = self._choose_best_feature_entropy(X, y)\n",
    "        elif self.criterion == 'gini':\n",
    "            best_feature_index, best_threshold = self._choose_best_feature_gini(X, y)\n",
    "        else: # GAIN RATIO\n",
    "            best_feature_index, best_threshold = self._choose_best_feature_gain_ratio(X, y)\n",
    "        return best_feature_index, best_threshold\n",
    "\n",
    "    def _choose_best_feature_entropy(self, X, y):\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "        best_info_gain = -1\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        H_parent = self._entropy(y)\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            # ordena as amostras com base no valor do atributo\n",
    "            X_feature = X[:, feature_index]\n",
    "            sorted_idx = X_feature.argsort()\n",
    "            X_sorted = X[sorted_idx]\n",
    "            y_sorted = y[sorted_idx]\n",
    "\n",
    "            # encontra os possíveis pontos de divisão\n",
    "            split_idx = np.where(y_sorted[:-1] != y_sorted[1:])[0] + 1\n",
    "            if len(split_idx) == 0:\n",
    "                continue\n",
    "\n",
    "            # calcula a entropia para cada ponto de divisão\n",
    "            for threshold in X_sorted[split_idx]:\n",
    "                y_left = y_sorted[X_sorted <= threshold]\n",
    "                y_right = y_sorted[X_sorted > threshold]\n",
    "                n_left = len(y_left)\n",
    "                n_right = len(y_right)\n",
    "                H_children = (n_left / n_samples) * self._entropy(y_left) + (n_right / n_samples) * self._entropy(y_right)\n",
    "                info_gain = H_parent - H_children\n",
    "\n",
    "                # atualiza o melhor atributo e ponto de divisão\n",
    "                if info_gain > best_info_gain:\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "                    best_info_gain = info_gain\n",
    "\n",
    "        return best_feature_index, best_threshold\n",
    "\n",
    "\n",
    "\n",
    "    def _choose_best_feature_gini(self, X, y):\n",
    "        best_score = 1.0\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X[:, feature_index] <= threshold\n",
    "                y_left = y[left_indices]\n",
    "                y_right = y[~left_indices]\n",
    "\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "\n",
    "                left_probabilities = np.array([len(y_left[y_left == c]) / len(y_left) for c in range(self.n_classes)])\n",
    "                right_probabilities = np.array([len(y_right[y_right == c]) / len(y_right) for c in range(self.n_classes)])\n",
    "\n",
    "                score = self._gini_impurity(left_probabilities, right_probabilities, len(y_left), len(y_right))\n",
    "\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature_index, best_threshold\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def _choose_best_feature_gain_ratio(self, X, y):\n",
    "        # escolhe o melhor atributo com base na razão de ganho\n",
    "        best_feature = None\n",
    "        best_gain_ratio = -1\n",
    "        \n",
    "        # calcula o ganho de informação de cada atributo\n",
    "        for feature_index in range(self.n_features):\n",
    "            gain_ratio = self._information_gain_ratio(X, y, feature_index)\n",
    "            if gain_ratio > best_gain_ratio:\n",
    "                best_feature = feature_index\n",
    "                best_gain_ratio = gain_ratio\n",
    "        \n",
    "        return best_feature\n",
    "\n",
    "\n",
    "    def _most_common_class(self, y):\n",
    "        # calcula a classe mais frequente\n",
    "        counter = collections.Counter(y)\n",
    "        most_common = counter.most_common(1)[0][0]\n",
    "        return most_common\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self._print_tree(self.tree)\n",
    "\n",
    "    def _print_tree(self, node, depth=0):\n",
    "        if node is None:\n",
    "            return \"\"\n",
    "        \n",
    "        prefix = \"  \" * depth\n",
    "        if node.value is not None:\n",
    "            return prefix + \"Class: \" + str(node.value) + \"\\n\"\n",
    "        \n",
    "        feature_name = \"Feature \" + str(node.feature_index)\n",
    "        threshold = \"Threshold: \" + str(node.threshold)\n",
    "        left_subtree = self._print_tree(node.left, depth + 1)\n",
    "        right_subtree = self._print_tree(node.right, depth + 1)\n",
    "\n",
    "        return prefix + feature_name + \", \" + threshold + \"\\n\" + \\\n",
    "               prefix + \"Left:\\n\" + left_subtree + \\\n",
    "               prefix + \"Right:\\n\" + right_subtree\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # exemplo de conjunto de dados\n",
    "    X_train = np.array([[1, 2], [2, 1], [2, 3], [3, 2], [4, 2], [3, 3], [2, 2], [3, 1]])\n",
    "    y_train = np.array([0, 0, 1, 1, 1, 0, 0, 1])\n",
    "\n",
    "\n",
    "    X_test = np.array([[1, 1], [4, 3], [3, 4], [2, 1]])\n",
    "\n",
    "    # Create the decision tree classifier with chosen hyperparameters\n",
    "    dtc = DecisionTreeClassifier(max_depth=3, min_samples_split=2, min_samples_leaf=1, criterion='gini', prune=None, size=None, independence=None)\n",
    "\n",
    "    # Train the model with the training dataset\n",
    "    dtc.fit(X_train, y_train)\n",
    "\n",
    "    # Print the tree\n",
    "    print(dtc)\n",
    "\n",
    "\n",
    "    # previsão das classes dos dados de teste\n",
    "    y_pred = dtc.predict(X_test)\n",
    "\n",
    "    # fazer reduced error pruning na decision tree\n",
    "    #dtc.reduced_error_pruning(X_val, y_val)\n",
    "\n",
    "    # fazer pessimistic error pruning na decision tree\n",
    "    #dtc.pessimistic_error_pruning(X_val, y_val)\n",
    "\n",
    "\n",
    "    # compara as classes reais com as classes previstas\n",
    "    #print(\"Classes reais: \", [1, 1, 0, 0])\n",
    "    print(\"Classes previstas: \", y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
